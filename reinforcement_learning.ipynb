{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Point of this notebook is simply to observe (withotu any decision making) the performance across a matrix of window skips and window lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampleddetection.environment.datastructures import Action, State\n",
    "from sampleddetection.environment.model import Environment\n",
    "from sampleddetection.datastructures.flowsession import SampledFlowSession\n",
    "from sampleddetection.common_lingo import Attack\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Make sure these are reloaded when cells are rerun\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the environment\n",
    "# From Microsecond to dekasecond\n",
    "window_skips    = np.logspace(-6, 1, 1, dtype=float) # DEBUG: This is like this while we test lengths \n",
    "window_lengths  = np.logspace(-6, -3, 4, dtype=float)\n",
    "#window_lengths  = 2*np.linspace(0.01, , 3, dtype=float)\n",
    "\n",
    "# Reinforcement Learning Training parameters\n",
    "batch_size      = 16\n",
    "simultaneous_simulations = 4\n",
    "agent_hidden_dim = 64\n",
    "\n",
    "multiclass_classifcation = True # If false, we label all attacks as 1\n",
    "csv_path = './data/Wednesday.csv'\n",
    "dataset_dir    = './data/precalc_windows/'\n",
    "dataset_filename = 'ws_{}_wl_{}.csv'\n",
    "desired_features = [\n",
    "            # Debugging info\n",
    "            #\"start_ts\",\n",
    "            #\"start_timestamp\",\n",
    "            #\"end_timestamp\",\n",
    "            #\"tot_fwd_pkts\",\n",
    "            #\"tot_bwd_pkts\",\n",
    "            # Non debugging\n",
    "            \"label\",\n",
    "            \"fwd_pkt_len_max\",\n",
    "            \"fwd_pkt_len_min\",\n",
    "            \"fwd_pkt_len_mean\",\n",
    "            \"bwd_pkt_len_max\",\n",
    "            \"bwd_pkt_len_min\",\n",
    "            \"bwd_pkt_len_mean\",\n",
    "            \"flow_byts_s\",\n",
    "            \"flow_pkts_s\",\n",
    "            \"flow_iat_mean\",\n",
    "            \"flow_iat_max\",\n",
    "            \"flow_iat_min\",\n",
    "            \"fwd_iat_mean\",\n",
    "            \"fwd_iat_max\",\n",
    "            \"fwd_iat_min\",\n",
    "            \"bwd_iat_max\",\n",
    "            \"bwd_iat_min\",\n",
    "            \"bwd_iat_mean\",\n",
    "            \"pkt_len_min\",\n",
    "            \"pkt_len_max\",\n",
    "            \"pkt_len_mean\",\n",
    "]\n",
    "attacks_to_detect = [\n",
    "    Attack.SLOWLORIS,\n",
    "    Attack.SLOWHTTPTEST,\n",
    "    Attack.HULK,\n",
    "    Attack.GOLDENEYE,\n",
    "    #Attack.HEARTBLEED. # Takes to long find in dataset.\n",
    "]\n",
    "\n",
    "# Use product to get a matrix of combinations\n",
    "options_matrix = list(product(window_skips, window_lengths))\n",
    "print(f\"Working with {len(options_matrix)} permutaitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Load dataset\n",
    "from sampleddetection.samplers.window_sampler import NoReplacementSampler\n",
    "from sampleddetection.writers.convenience import save_flows_to_csv\n",
    "from sampleddetection.readers.readers import CSVReader\n",
    "from sampleddetection.common_lingo import Attack, ATTACK_TO_STRING\n",
    "from sampleddetection.environment.models import Agent\n",
    "from joblib import load\n",
    "\n",
    "# TODO: Perhaps make it so the enviornment can tell use when we are sampling around the same areas. \n",
    "environments = [ Environment(sampler) for i in range(simultaneous_simulations)]\n",
    "agent_input_dim = len(desired_features)\n",
    "baseline_classifier = load(\"models/detection_model.joblib\")\n",
    "agent = Agent(agent_input_dim, agent_hidden_dim, 2, desired_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the simulations\n",
    "for i in range(simultaneous_simulations):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampleddetection.datastructures.flow import Flow\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def generate_sessions(ws: float, wl: float) -> List[Tuple[Tuple,Flow]]:\n",
    "    \"\"\"Ensure we get a balanced sampling from the large dataset.\"\"\"\n",
    "    cur_amnt = 0\n",
    "    flows: List[Tuple,Flow] = []\n",
    "    if multiclass_classifcation:\n",
    "        count_per_class = {attack: 0 for attack in attacks_to_detect}\n",
    "        count_per_class[Attack.BENIGN] = 0\n",
    "        total_amount = min_necessary_classes*(len(attacks_to_detect)+1)\n",
    "        inner_bar = tqdm(total=total_amount,desc=f'Generating ws: {ws}- wl: {wl} flow',leave=False)\n",
    "    else:\n",
    "        count_per_class = {Attack.BENIGN: 0, Attack.GENERAL: 0}\n",
    "        total_amount = min_necessary_classes*2\n",
    "        inner_bar = tqdm(total=total_amount,desc=f'Generating ws: {ws}- wl: {wl} flow',leave=False)\n",
    "\n",
    "    enough_samples_per_class = {class_name: False for class_name in count_per_class.keys()}\n",
    "    sampler.clear_memory()\n",
    "    while all(enough_samples_per_class.values()) == False:\n",
    "\n",
    "        flow_sesh =  sampler.sample(winskip=ws,winlen=wl).flow_sesh\n",
    "        # Count the distributions\n",
    "        label_distributions = flow_sesh.flow_label_distribution()\n",
    "        # For now just predict binary attack-benining\n",
    "        for kflow, flow in flow_sesh.flows.items() : \n",
    "            if multiclass_classifcation:\n",
    "                label = flow.label\n",
    "            else:\n",
    "                label = Attack.GENERAL if flow.label != Attack.BENIGN else Attack.BENIGN\n",
    "\n",
    "            if label not in count_per_class or count_per_class[label] >= min_necessary_classes:\n",
    "                continue # Dont over add\n",
    "\n",
    "            count_per_class[label] += 1\n",
    "            if count_per_class[label] == min_necessary_classes:\n",
    "                enough_samples_per_class[label] = True\n",
    "                \n",
    "            flows.append((kflow,flow))\n",
    "\n",
    "            inner_bar.update(1)\n",
    "    return flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flows = {}\n",
    "# Set random seeds:\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "import csv\n",
    "\n",
    "\n",
    "# Generate the datasets\n",
    "for ws, wl in tqdm(options_matrix,desc='Creating datasets'):\n",
    "    # Check if datasets exists\n",
    "    flows = {f\"ws:{ws}-ws:{wl}\" : []}\n",
    "    target_name = os.path.join(dataset_dir,dataset_filename.format(ws, wl))\n",
    "    if os.path.exists(target_name):\n",
    "        print(f\"Will later be Loading {dataset_filename.format(ws, wl)} from {dataset_dir}\")\n",
    "        continue\n",
    "    sesh = generate_sessions(ws,wl)\n",
    "\n",
    "    ds_path = os.path.join(dataset_dir,dataset_filename.format(ws, wl))\n",
    "    save_flows_to_csv(sesh, ds_path, desired_features=desired_features, samples_per_class=samples_per_class,overwrite=True, multiclass=multiclass_classifcation)\n",
    "    \n",
    "!notify-send done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the sampling windows to ensure they are disjoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sampleddetection.common_lingo import Attack\n",
    "from sampleddetection.datastructures.flow import Flow\n",
    "\n",
    "max_y = 100\n",
    "y_increase = 100/len(flows)\n",
    "\n",
    "time_windows = [f.time_window for f in flows]\n",
    "\n",
    "# Ideally these two should be the same.\n",
    "time_windows = sorted(time_windows, key=lambda tw: tw.start)\n",
    "time_windows_end_wise = sorted(time_windows, key=lambda tw: tw.end)\n",
    "    \n",
    "# Actuallly plot them\n",
    "for tw in time_windows:\n",
    "    plt.plot([tw.start,tw.end],[i*y_increase,i(y_increase)],label='start', marker='o')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Time Windows')\n",
    "plt.xlim(time_windows[0].start,1time_windows_end_wise[-1].end)\n",
    "plt.title(\"Timeline of Windows\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PreCalced datasets\n",
    "for ws, wl in tqdm(options_matrix,desc='Loading datasets'):\n",
    "    target_name = os.path.join(dataset_dir,dataset_filename.format(ws, wl))\n",
    "    if not os.path.exists(target_name):\n",
    "        print(f\"Could not find {target_name}\")\n",
    "        raise FileNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model on Different Schedules\n",
    "\n",
    "We will use the matrix of different parameters to see how the training changes performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a function that will take flows calculated/loaded up above and will train the model. \n",
    "# It will return data of  the training and testing results to later be plotted in a loop that will call it\n",
    "import pandas as pd\n",
    "from sampleddetection.util.data import clean_dataset,train_classifier_XGBoost,train_multinary_classier_XGBoost\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = [\n",
    "            \"label\",\n",
    "            \"fwd_pkt_len_max\",\n",
    "            \"fwd_pkt_len_min\",\n",
    "            \"fwd_pkt_len_mean\",\n",
    "            \"bwd_pkt_len_max\",\n",
    "            \"bwd_pkt_len_min\",\n",
    "            \"bwd_pkt_len_mean\",\n",
    "            \"flow_byts_s\",\n",
    "            \"flow_pkts_s\",\n",
    "            \"flow_iat_mean\",\n",
    "            \"flow_iat_max\",\n",
    "            \"flow_iat_min\",\n",
    "            \"fwd_iat_mean\",\n",
    "            \"fwd_iat_max\",\n",
    "            \"fwd_iat_min\",\n",
    "          #  \"bwd_iat_max\",\n",
    "            \"bwd_iat_min\",\n",
    "            \"bwd_iat_mean\",\n",
    "            \"pkt_len_min\",\n",
    "            \"pkt_len_max\",\n",
    "            \"pkt_len_mean\",\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_performance(df: pd.DataFrame, ws: float, wl: float) -> Dict:\n",
    "    \"\"\"Evaluate the performance of the model with the given dataset.\"\"\"\n",
    "    # Clean the dataset\n",
    "    df_ddos = clean_dataset(df,features, attacks_to_detect, (multiclass_classifcation == False))\n",
    "\n",
    "    # Train the Model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_ddos.drop(columns=[\"label\"]), df_ddos[\"label\"], test_size=0.3\n",
    "    )\n",
    "\n",
    "    if multiclass_classifcation:\n",
    "      mode, evals =  train_multinary_classier_XGBoost(X_train,  y_train,X_test, y_test)\n",
    "\n",
    "    else:\n",
    "      mode, evals = train_classifier_XGBoost(X_train,  y_train,X_test, y_test)\n",
    "\n",
    "    return evals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the outer loop that will vall evaluete_performance\n",
    "accuracies = []\n",
    "log_losses = []\n",
    "roc_aucs = []\n",
    "for ws, wl in tqdm(options_matrix,desc='Evaluating datasets'):\n",
    "    target_name = os.path.join(dataset_dir,dataset_filename.format(ws, wl))\n",
    "    if not os.path.exists(target_name):\n",
    "        print(f\"Could not find {target_name}\")\n",
    "        raise FileNotFoundError\n",
    "    # Load the data\n",
    "    df = pd.read_csv(target_name)\n",
    "    # Evaluate the data\n",
    "    metrics = evaluate_performance(df, ws, wl)\n",
    "    accuracies.append(metrics[\"accuracy\"])\n",
    "    log_losses.append(metrics[\"log_loss\"])\n",
    "    #roc_aucs.append(metrics[\"roc_auc\"])\n",
    "\n",
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Graph disregarding winskips\n",
    "\n",
    "# Setup the figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "wl_notation = [f\"{wl:.2e}\" for wl in window_lengths]\n",
    "\n",
    "# Plot dotted curve  with seaborn\n",
    "sns.lineplot(x=wl_notation, y=accuracies, ax=ax[0], label=\"Accuracy\")\n",
    "sns.lineplot(x=wl_notation, y=log_losses, ax=ax[1], label=\"Log Loss\")\n",
    "\n",
    "# Set the labels\n",
    "ax[0].set_xlabel(\"Window Length\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].set_title(\"Accuracy vs Window Length\")\n",
    "ax[0].legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat Map Plotting(FOR BINARY CLASSIFICATION ONLY FOR NOW)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import LogFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "#winskips = [f\"{ws:.2f}\" for ws, wl in options_matrix]\n",
    "#winlens = [f\"{wl:.2f}\" for ws, wl in options_matrix]\n",
    "winskips = [f\"{i:.2e}\" for i in np.logspace(-6, 1, 1)]\n",
    "winlens = [f\"{i:.2e}\" for i in np.logspace(-3, 1, 4)]\n",
    "# Format these with scientific (1e-6) notation\n",
    "\n",
    "fig, ax = plt.subplots(3,1,figsize=(5,10))\n",
    "\n",
    "#formatter = LogFormatter(10, labelOnlyBase=False)\n",
    "\n",
    "print(winskips)\n",
    "\n",
    "sns.heatmap(np.array(accuracies).reshape(1,4),ax=ax[0],annot=True,fmt=\".2f\", xticklabels=winskips, yticklabels=winlens)\n",
    "ax[0].set_title(\"Accuracy\")\n",
    "ax[0].set_xlabel(\"Window Length\")\n",
    "ax[0].set_ylabel(\"Window Skip\")\n",
    "\n",
    "sns.heatmap(np.array(log_losses).reshape(1,4),ax=ax[1],annot=True,fmt=\".2f\",xticklabels=winskips, yticklabels=winlens)\n",
    "ax[1].set_title(\"Log Loss\")\n",
    "ax[1].set_xlabel(\"Window Length\")\n",
    "ax[1].set_ylabel(\"Window Skip\")\n",
    "\n",
    "#sns.heatmap(np.array(roc_aucs).reshape(1,4),ax=ax[2],annot=True,fmt=\".2f\",xticklabels=winskips, yticklabels=winlens)\n",
    "#ax[2].set_title(\"ROC AUC\")\n",
    "#ax[2].set_xlabel(\"Window Length\")\n",
    "#ax[2].set_ylabel(\"Window Skip\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
